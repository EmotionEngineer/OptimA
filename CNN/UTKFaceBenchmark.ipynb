{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d60b81b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T08:01:18.824291Z",
     "iopub.status.busy": "2025-05-08T08:01:18.823677Z",
     "iopub.status.idle": "2025-05-08T08:01:32.767651Z",
     "shell.execute_reply": "2025-05-08T08:01:32.767031Z"
    },
    "papermill": {
     "duration": 13.948966,
     "end_time": "2025-05-08T08:01:32.769149",
     "exception": false,
     "start_time": "2025-05-08T08:01:18.820183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 08:01:20.454667: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746691280.650969      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746691280.707946      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, Input, Conv2D, MaxPooling2D, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess_input\n",
    "from tensorflow.keras.datasets import cifar10, mnist\n",
    "from tensorflow.keras.optimizers import Adam, SGD, AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import rotate\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# --- Configuration ---\n",
    "RAND_SEED = 42\n",
    "NUM_RUNS = 1\n",
    "EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "BATCH_SIZE = 16\n",
    "ROTATION_ANGLE_MAX = 90\n",
    "LEARNING_RATE = 1e-5 # For fine-tuning ResNet\n",
    "\n",
    "# UTKFace specific\n",
    "UTKFACE_DIR = '/kaggle/input/utkface-new/UTKFace'\n",
    "IMAGE_SIZE_RESNET = (224, 224)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Sets random seeds for reproducibility.\"\"\"\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # For full determinism, uncomment below (can slow down execution)\n",
    "    # tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    # tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "set_seed(RAND_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad8955d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-08T08:01:32.775041Z",
     "iopub.status.busy": "2025-05-08T08:01:32.774497Z",
     "iopub.status.idle": "2025-05-08T08:01:32.824049Z",
     "shell.execute_reply": "2025-05-08T08:01:32.823476Z"
    },
    "papermill": {
     "duration": 0.053619,
     "end_time": "2025-05-08T08:01:32.825168",
     "exception": false,
     "start_time": "2025-05-08T08:01:32.771549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Custom Activation Functions ---\n",
    "class OptimA(Layer):\n",
    "    \"\"\"Custom Optimal Activation function.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(OptimA, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(name='alpha', shape=(), initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=(), initializer=tf.keras.initializers.Constant(0.5), trainable=True)\n",
    "        self.gamma = self.add_weight(name='gamma', shape=(), initializer='ones', trainable=True)\n",
    "        self.delta = self.add_weight(name='delta', shape=(), initializer=tf.keras.initializers.Constant(0.5), trainable=True)\n",
    "        self.lambda_ = self.add_weight(name='lambda', shape=(), initializer='ones', trainable=True)\n",
    "        super(OptimA, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        term1 = self.alpha * tf.math.tanh(self.beta * x)\n",
    "        term2 = self.gamma * tf.math.softplus(self.delta * x) * tf.math.sigmoid(self.lambda_ * x)\n",
    "        return term1 + term2\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(OptimA, self).get_config()\n",
    "        return config\n",
    "\n",
    "class OptimALinear(Layer):\n",
    "    \"\"\"Custom Optimal Activation function (Linear Approximation).\"\"\"\n",
    "    def __init__(self, epsilon=1e-5, **kwargs):\n",
    "        super(OptimALinear, self).__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(name='alpha', shape=(), initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=(), initializer=tf.keras.initializers.Constant(0.5), trainable=True)\n",
    "        self.gamma = self.add_weight(name='gamma', shape=(), initializer='ones', trainable=True)\n",
    "        self.delta = self.add_weight(name='delta', shape=(), initializer=tf.keras.initializers.Constant(0.5), trainable=True)\n",
    "        self.lambda_ = self.add_weight(name='lambda', shape=(), initializer='ones', trainable=True)\n",
    "        super(OptimALinear, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        term1 = self.alpha * tf.clip_by_value(self.beta * x, -1, 1)\n",
    "        term2 = self.gamma * (tf.maximum(0.0, self.delta * x) + self.epsilon) * (0.5 + 0.25 * self.lambda_ * x)\n",
    "        return term1 + term2\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(OptimALinear, self).get_config()\n",
    "        config.update({'epsilon': self.epsilon})\n",
    "        return config\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "def load_and_prepare_image_classification_data(load_func, dataset_name):\n",
    "    \"\"\"Loads and prepares standard image classification datasets (MNIST, CIFAR-10).\"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = load_func()\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "    if dataset_name == \"MNIST\" or dataset_name == \"RotatedMNIST\":\n",
    "        x_train = np.expand_dims(x_train, -1)\n",
    "        x_test = np.expand_dims(x_test, -1)\n",
    "    elif dataset_name == \"CIFAR-10\": # Ensure 3 channels for CIFAR-10\n",
    "        if x_train.shape[-1] == 1:\n",
    "            x_train = tf.image.grayscale_to_rgb(tf.convert_to_tensor(x_train)).numpy()\n",
    "            x_test = tf.image.grayscale_to_rgb(tf.convert_to_tensor(x_test)).numpy()\n",
    "\n",
    "    num_classes = len(np.unique(np.concatenate((y_train.flatten(), y_test.flatten()))))\n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "    return x_train, y_train, x_test, y_test, num_classes\n",
    "\n",
    "def load_and_prepare_rotated_mnist(max_angle=ROTATION_ANGLE_MAX, seed=None):\n",
    "    \"\"\"Loads MNIST, rotates images, and sets the angle as the regression target.\"\"\"\n",
    "    if seed is not None: np.random.seed(seed)\n",
    "    (x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "    train_angles = np.random.uniform(-max_angle, max_angle, size=len(x_train))\n",
    "    y_train_reg = train_angles / max_angle\n",
    "    x_train_rot = np.array([rotate(img, angle, reshape=False, mode='nearest', order=1) for img, angle in zip(x_train, train_angles)])\n",
    "\n",
    "    test_angles = np.random.uniform(-max_angle, max_angle, size=len(x_test))\n",
    "    y_test_reg = test_angles / max_angle\n",
    "    x_test_rot = np.array([rotate(img, angle, reshape=False, mode='nearest', order=1) for img, angle in zip(x_test, test_angles)])\n",
    "\n",
    "    x_train_rot = x_train_rot.astype('float32') / 255.0\n",
    "    x_test_rot = x_test_rot.astype('float32') / 255.0\n",
    "    x_train_rot = np.expand_dims(x_train_rot, -1)\n",
    "    x_test_rot = np.expand_dims(x_test_rot, -1)\n",
    "    return x_train_rot, y_train_reg, x_test_rot, y_test_reg\n",
    "\n",
    "def parse_utkface_metadata(data_dir, seed=None):\n",
    "    \"\"\"Parses UTKFace filenames to extract image paths and ages.\"\"\"\n",
    "    print(f\"Parsing UTKFace metadata from: {data_dir}\")\n",
    "    if not os.path.isdir(data_dir):\n",
    "        raise FileNotFoundError(f\"UTKFace directory not found or is not a directory: {data_dir}\")\n",
    "\n",
    "    image_paths, ages = [], []\n",
    "    corrupted_files = 0\n",
    "    filenames = os.listdir(data_dir)\n",
    "    if not filenames: raise ValueError(f\"No files found in UTKFace directory: {data_dir}\")\n",
    "\n",
    "    for filename in filenames:\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            try:\n",
    "                age = int(filename.split('_')[0])\n",
    "                if 1 <= age <= 116: # UTKFace typical age range\n",
    "                    image_paths.append(os.path.join(data_dir, filename))\n",
    "                    ages.append(float(age))\n",
    "                else: corrupted_files += 1\n",
    "            except (IndexError, ValueError): corrupted_files += 1\n",
    "    \n",
    "    if not image_paths:\n",
    "        raise ValueError(f\"No valid image files parsed in {data_dir}. Found {len(filenames)} files, skipped {corrupted_files}.\")\n",
    "    print(f\"Parsed {len(image_paths)} image paths. Skipped {corrupted_files} files.\")\n",
    "\n",
    "    if seed is not None: random.seed(seed) # Seed for shuffling\n",
    "    combined = list(zip(image_paths, ages))\n",
    "    random.shuffle(combined)\n",
    "    if combined: image_paths[:], ages[:] = zip(*combined) # Unpack shuffled data\n",
    "    else: image_paths, ages = [], [] # Handle empty case after filtering\n",
    "    return image_paths, ages\n",
    "\n",
    "def load_and_preprocess_image_tf(path, label, target_size):\n",
    "    \"\"\"Loads and preprocesses a single image for a tf.data.Dataset pipeline.\"\"\"\n",
    "    try:\n",
    "        image_string = tf.io.read_file(path)\n",
    "        image = tf.image.decode_image(image_string, channels=3, expand_animations=False) # Ensure 3 channels\n",
    "        image = tf.image.resize(image, target_size)\n",
    "        image.set_shape([target_size[0], target_size[1], 3]) # Explicitly set shape\n",
    "        image = resnet_preprocess_input(image) # Apply ResNet specific preprocessing\n",
    "        return image, label\n",
    "    except Exception as e:\n",
    "        # tf.print(f\"Error processing image {path}: {e}. Returning zeros.\") # Use tf.print for TF graph mode\n",
    "        # Return a \"dummy\" image and label if processing fails\n",
    "        return tf.zeros((*target_size, 3), dtype=tf.float32), tf.cast(label, dtype=tf.float32)\n",
    "\n",
    "def create_tf_dataset(image_paths, labels, batch_size_local, target_size, is_training=True, buffer_size_shuffle=1000):\n",
    "    \"\"\"Creates a tf.data.Dataset from image paths and labels for efficient data loading.\"\"\"\n",
    "    if not image_paths: return None\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(image_paths), list(labels)))\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=min(len(image_paths), buffer_size_shuffle), reshuffle_each_iteration=True)\n",
    "\n",
    "    dataset = dataset.map(lambda p, l: load_and_preprocess_image_tf(p, l, target_size),\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE) # Parallelize image loading\n",
    "    \n",
    "    # Optional: filter out images that failed to load (if load_and_preprocess_image_tf returns a distinguishable dummy)\n",
    "    # dataset = dataset.filter(lambda image, label: not tf.reduce_all(tf.equal(image, tf.zeros_like(image))))\n",
    "\n",
    "    dataset = dataset.batch(batch_size_local)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE) # Prefetch batches for performance\n",
    "    return dataset\n",
    "\n",
    "# --- Helper to get activation layer ---\n",
    "def get_activation(act_config):\n",
    "    \"\"\"Instantiates or returns an activation layer/name.\"\"\"\n",
    "    if isinstance(act_config, str): return tf.keras.layers.Activation(act_config)\n",
    "    elif isinstance(act_config, Layer): # If it's an already instantiated custom layer\n",
    "        if type(act_config) == OptimA: return OptimA() # Create new instance\n",
    "        if type(act_config) == OptimALinear: return OptimALinear() # Create new instance\n",
    "        return act_config # Fallback: reuse instance (less ideal)\n",
    "    elif isinstance(act_config, type) and issubclass(act_config, Layer): return act_config() # Instantiate class type\n",
    "    else: raise ValueError(f\"Unsupported activation: {act_config}\")\n",
    "\n",
    "# --- Model Building ---\n",
    "def build_cnn_model(input_shape, output_units, activation_func_config, task_type):\n",
    "    \"\"\"Builds a simple CNN model.\"\"\"\n",
    "    if isinstance(activation_func_config, str): activation_name = activation_func_config\n",
    "    elif hasattr(activation_func_config, '__name__'): activation_name = activation_func_config.__name__\n",
    "    else: activation_name = activation_func_config.__class__.__name__\n",
    "\n",
    "    model = Sequential(name=f\"SimpleCNN_{activation_name}_{task_type}\")\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Conv2D(32, (3,3), padding='same')); model.add(get_activation(activation_func_config)); model.add(MaxPooling2D((2,2))); model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3,3), padding='same')); model.add(get_activation(activation_func_config)); model.add(MaxPooling2D((2,2))); model.add(Dropout(0.2))\n",
    "    model.add(Flatten()); model.add(Dense(128)); model.add(get_activation(activation_func_config)); model.add(Dropout(0.3))\n",
    "    if task_type == \"classification\": model.add(Dense(output_units, activation='softmax'))\n",
    "    elif task_type == \"regression\": model.add(Dense(output_units, activation='linear'))\n",
    "    else: raise ValueError(f\"Unsupported task: {task_type}\")\n",
    "    return model\n",
    "\n",
    "def build_resnet50_regression_model(input_shape, activation_func_config):\n",
    "    \"\"\"Builds a ResNet-50 based model for regression with a custom activation in the head.\"\"\"\n",
    "    if isinstance(activation_func_config, str): activation_name = activation_func_config\n",
    "    elif hasattr(activation_func_config, '__name__'): activation_name = activation_func_config.__name__\n",
    "    else: activation_name = activation_func_config.__class__.__name__\n",
    "\n",
    "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    base_model.trainable = True # Fine-tune all layers of ResNet\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, name='fc1')(x)\n",
    "    x = get_activation(activation_func_config)(x) # Apply custom/standard activation\n",
    "    x = Dropout(0.3)(x)\n",
    "    output_tensor = Dense(1, activation='linear', name='age_output')(x) # Linear output for regression\n",
    "    model = Model(inputs=base_model.input, outputs=output_tensor, name=f\"ResNet50_{activation_name}_Reg\")\n",
    "    return model\n",
    "\n",
    "# --- Experiment Execution ---\n",
    "def run_experiment(datasets_config, activations_config, optimizers_config, num_runs, epochs_local, batch_size_local, patience_local):\n",
    "    \"\"\"Runs the benchmarking experiments across datasets, activations, and optimizers.\"\"\"\n",
    "    results = {}\n",
    "    histories = {}\n",
    "\n",
    "    for run_idx in range(num_runs):\n",
    "        print(f\"\\n--- Starting Run {run_idx + 1}/{num_runs} ---\")\n",
    "        current_seed = RAND_SEED + run_idx\n",
    "        set_seed(current_seed)\n",
    "\n",
    "        for dataset_name, dataset_cfg in datasets_config.items():\n",
    "            print(f\"\\nDataset: {dataset_name} (Task: {dataset_cfg['task']})\")\n",
    "            if dataset_name not in results:\n",
    "                results[dataset_name], histories[dataset_name] = {}, {}\n",
    "\n",
    "            task_type, model_arch = dataset_cfg['task'], dataset_cfg.get('model_type', 'cnn')\n",
    "            # Initialize data-related variables\n",
    "            train_src, val_src, test_eval_src = None, None, None\n",
    "            steps_epoch, val_steps = None, None\n",
    "            input_s, output_u = None, None\n",
    "            loss_f, eval_m, primary_m_name, primary_m_idx = None, None, None, None\n",
    "\n",
    "            # Load and prepare data based on dataset type and task\n",
    "            if dataset_cfg[\"type\"] == \"image\" and task_type == \"classification\":\n",
    "                x_tr, y_tr, x_te, y_te, n_classes = load_and_prepare_image_classification_data(dataset_cfg[\"load_func\"], dataset_name)\n",
    "                input_s, output_u = x_tr.shape[1:], n_classes\n",
    "                loss_f, eval_m, primary_m_name, primary_m_idx = 'categorical_crossentropy', ['accuracy'], 'accuracy', 1\n",
    "                train_src, test_eval_src = (x_tr, y_tr), (x_te, y_te) # Use NumPy arrays directly\n",
    "            elif dataset_cfg[\"type\"] == \"image\" and task_type == \"regression\":\n",
    "                if dataset_name == \"UTKFaceAge\":\n",
    "                    paths, labels = parse_utkface_metadata(UTKFACE_DIR, current_seed)\n",
    "                    if not paths: print(f\"Skipping UTKFaceAge for run {run_idx+1}, no data parsed.\"); continue\n",
    "                    \n",
    "                    # Split data (e.g., 70% train, 15% val, 15% test)\n",
    "                    n_total = len(paths); n_tr = int(n_total * 0.7); n_v = int(n_total * 0.15)\n",
    "                    tr_p, tr_l = paths[:n_tr], labels[:n_tr]\n",
    "                    v_p, v_l = paths[n_tr:n_tr+n_v], labels[n_tr:n_tr+n_v]\n",
    "                    te_p, te_l = paths[n_tr+n_v:], labels[n_tr+n_v:]\n",
    "                    print(f\"UTKFace splits: Train: {len(tr_p)}, Validation: {len(v_p)}, Test: {len(te_p)}\")\n",
    "                    if not tr_p or not v_p or not te_p: print(f\"UTKFace data split resulted in an empty set. Skipping.\"); continue\n",
    "\n",
    "                    # Create tf.data.Dataset for efficient loading\n",
    "                    train_src = create_tf_dataset(tr_p, tr_l, batch_size_local, IMAGE_SIZE_RESNET, is_training=True)\n",
    "                    val_src = create_tf_dataset(v_p, v_l, batch_size_local, IMAGE_SIZE_RESNET, is_training=False)\n",
    "                    test_eval_src = create_tf_dataset(te_p, te_l, batch_size_local, IMAGE_SIZE_RESNET, is_training=False)\n",
    "                    if not train_src or not val_src or not test_eval_src: print(\"Failed to create UTKFace tf.data.Dataset. Skipping.\"); continue\n",
    "                    \n",
    "                    # Calculate steps for training and validation\n",
    "                    steps_epoch = (len(tr_p) + batch_size_local -1) // batch_size_local # Ceiling division\n",
    "                    val_steps = (len(v_p) + batch_size_local -1) // batch_size_local   # Ceiling division\n",
    "                    input_s = (*IMAGE_SIZE_RESNET, 3)\n",
    "                elif dataset_name == \"RotatedMNIST\":\n",
    "                    x_tr_rot, y_tr_reg, x_te_rot, y_te_reg = dataset_cfg[\"load_func\"](seed=current_seed)\n",
    "                    if x_tr_rot.ndim == 3: x_tr_rot,x_te_rot = np.expand_dims(x_tr_rot,-1), np.expand_dims(x_te_rot,-1) # Add channel dim\n",
    "                    input_s = x_tr_rot.shape[1:]\n",
    "                    train_src, test_eval_src = (x_tr_rot, y_tr_reg), (x_te_rot, y_te_reg)\n",
    "                \n",
    "                output_u = 1 # Single output for regression\n",
    "                loss_f, eval_m, primary_m_name, primary_m_idx = 'mse', ['mae'], 'mae', 1 # MSE loss, MAE metric\n",
    "            else: raise ValueError(f\"Unsupported dataset configuration: {dataset_name}\")\n",
    "            if input_s is None: print(f\"Input shape not determined for {dataset_name}. Skipping this dataset iteration.\"); continue\n",
    "\n",
    "            # Iterate over optimizers and activations\n",
    "            for opt_name, opt_creator in optimizers_config.items():\n",
    "                print(f\"  Optimizer: {opt_name} (Run {run_idx + 1})\")\n",
    "                if opt_name not in results[dataset_name]: results[dataset_name][opt_name], histories[dataset_name][opt_name] = {}, {}\n",
    "                for act_name, act_cfg_val in activations_config.items():\n",
    "                    print(f\"    Activation: {act_name}\")\n",
    "                    if act_name not in results[dataset_name][opt_name]:\n",
    "                        results[dataset_name][opt_name][act_name] = {'loss': [], primary_m_name: [], 'time': [], 'params': []}\n",
    "                        histories[dataset_name][opt_name][act_name] = []\n",
    "\n",
    "                    tf.keras.backend.clear_session(); gc.collect(); set_seed(current_seed) # Reset state\n",
    "                    \n",
    "                    # Build model\n",
    "                    model = build_resnet50_regression_model(input_s, act_cfg_val) if model_arch == 'resnet50' \\\n",
    "                            else build_cnn_model(input_s, output_u, act_cfg_val, task_type)\n",
    "                    \n",
    "                    opt_instance = opt_creator() # Create fresh optimizer instance\n",
    "                    model.compile(optimizer=opt_instance, loss=loss_f, metrics=eval_m)\n",
    "                    \n",
    "                    # Define monitor metric for callbacks\n",
    "                    mon_metric = 'val_mae' if primary_m_name == 'mae' else ('val_accuracy' if task_type == 'classification' else 'val_loss')\n",
    "                    cbs = [EarlyStopping(monitor=mon_metric, patience=patience_local, restore_best_weights=True, verbose=1),\n",
    "                           ReduceLROnPlateau(monitor=mon_metric, factor=0.3, patience=patience_local//2, min_lr=1e-7, verbose=1)]\n",
    "                    \n",
    "                    start_t = time.time()\n",
    "                    # Prepare arguments for model.fit\n",
    "                    fit_args = {\"epochs\": epochs_local, \"callbacks\": cbs, \"verbose\": 1}\n",
    "                    if isinstance(train_src, tf.data.Dataset): # If using tf.data.Dataset\n",
    "                        fit_args.update({\"x\": train_src, \"validation_data\": val_src})\n",
    "                        if steps_epoch: fit_args[\"steps_per_epoch\"] = steps_epoch\n",
    "                        if val_steps: fit_args[\"validation_steps\"] = val_steps\n",
    "                    else: # If using NumPy arrays\n",
    "                        fit_args.update({\"x\": train_src[0], \"y\": train_src[1], \"batch_size\": batch_size_local, \"validation_split\": 0.2})\n",
    "                    \n",
    "                    hist = model.fit(**fit_args) # Train the model\n",
    "                    train_time = time.time() - start_t\n",
    "\n",
    "                    print(\"Evaluating model...\")\n",
    "                    # Evaluate model\n",
    "                    eval_res = model.evaluate(test_eval_src, verbose=0) if isinstance(test_eval_src, tf.data.Dataset) \\\n",
    "                               else model.evaluate(test_eval_src[0], test_eval_src[1], batch_size=batch_size_local, verbose=0)\n",
    "                    \n",
    "                    eval_l, eval_pm = eval_res[0], eval_res[primary_m_idx]\n",
    "                    n_params = model.count_params()\n",
    "\n",
    "                    # Store results\n",
    "                    res_dict = results[dataset_name][opt_name][act_name]\n",
    "                    res_dict['loss'].append(eval_l); res_dict[primary_m_name].append(eval_pm)\n",
    "                    res_dict['time'].append(train_time); res_dict['params'].append(n_params)\n",
    "                    histories[dataset_name][opt_name][act_name].append(hist.history)\n",
    "                    print(f\"      Loss: {eval_l:.4f}, {primary_m_name.capitalize()}: {eval_pm:.4f}, Time: {train_time:.2f}s, Params: {n_params}\")\n",
    "\n",
    "                    del model, opt_instance, hist; gc.collect() # Clean up\n",
    "            \n",
    "            # Clean up data after processing a dataset to free memory\n",
    "            if dataset_cfg[\"type\"] == \"image\" and task_type == \"classification\": del x_tr, y_tr, x_te, y_te\n",
    "            elif dataset_name == \"RotatedMNIST\": del x_tr_rot, y_tr_reg, x_te_rot, y_te_reg\n",
    "            del train_src, val_src, test_eval_src; gc.collect()\n",
    "    return results, histories\n",
    "\n",
    "# --- Aggregate and Display Results ---\n",
    "def aggregate_and_display_results(final_results, datasets_cfg_local, all_histories_local, num_runs_executed):\n",
    "    \"\"\"Aggregates results from multiple runs and displays them in a table and plots.\"\"\"\n",
    "    aggregated = {}\n",
    "    best_mae = {}  # To store best MAE for each regression dataset\n",
    "    \n",
    "    # First pass: aggregate all data and identify best MAE for regression tasks\n",
    "    for ds_name, opt_data in final_results.items():\n",
    "        aggregated[ds_name] = {}\n",
    "        task = datasets_cfg_local[ds_name]['task']\n",
    "        metric_k = 'accuracy' if task == 'classification' else 'mae'\n",
    "        \n",
    "        if task == 'regression':\n",
    "            best_mae[ds_name] = {'value': float('inf'), 'optimizer': None, 'activation': None}\n",
    "        \n",
    "        for opt_n, act_data in opt_data.items():\n",
    "            aggregated[ds_name][opt_n] = {}\n",
    "            for act_n, metrics_vals in act_data.items():\n",
    "                if not metrics_vals['loss']: # Handle case where no runs completed for this combo\n",
    "                    agg_data = {'Avg Loss': np.nan, 'Std Loss': np.nan, f'Avg {metric_k.capitalize()}': np.nan,\n",
    "                                f'Std {metric_k.capitalize()}': np.std(metrics_vals[metric_k]) if metrics_vals[metric_k] else np.nan,\n",
    "                                'Avg Time': np.nan, 'Std Time': np.nan, 'Avg Params': np.nan}\n",
    "                else:\n",
    "                    avg_mae = np.mean(metrics_vals['mae']) if 'mae' in metrics_vals else np.nan\n",
    "                    agg_data = {'Avg Loss': np.mean(metrics_vals['loss']), 'Std Loss': np.std(metrics_vals['loss']),\n",
    "                                f'Avg {metric_k.capitalize()}': avg_mae if metric_k == 'mae' else np.mean(metrics_vals[metric_k]),\n",
    "                                f'Std {metric_k.capitalize()}': np.std(metrics_vals[metric_k]),\n",
    "                                'Avg Time': np.mean(metrics_vals['time']), 'Std Time': np.std(metrics_vals['time']),\n",
    "                                'Avg Params': int(np.mean(metrics_vals['params'])) if metrics_vals['params'] else np.nan}\n",
    "                    \n",
    "                    # Track best MAE for regression tasks\n",
    "                    if task == 'regression' and 'mae' in metrics_vals and avg_mae < best_mae[ds_name]['value']:\n",
    "                        best_mae[ds_name]['value'] = avg_mae\n",
    "                        best_mae[ds_name]['optimizer'] = opt_n\n",
    "                        best_mae[ds_name]['activation'] = act_n\n",
    "                \n",
    "                aggregated[ds_name][opt_n][act_n] = agg_data\n",
    "    \n",
    "    # Create DataFrame for results\n",
    "    all_rows = []\n",
    "    for ds, o_data in aggregated.items():\n",
    "        task = datasets_cfg_local[ds]['task']\n",
    "        for o, a_data in o_data.items():\n",
    "            for a, m_vals in a_data.items():\n",
    "                row = {'Dataset': ds, 'Optimizer': o, 'Activation': a}\n",
    "                row.update(m_vals)\n",
    "                \n",
    "                # Add Best MAE marker for regression tasks\n",
    "                if task == 'regression':\n",
    "                    is_best = (best_mae.get(ds, {}).get('optimizer') == o and \n",
    "                              best_mae.get(ds, {}).get('activation') == a)\n",
    "                    row['Best MAE'] = '★' if is_best else ''\n",
    "                \n",
    "                all_rows.append(row)\n",
    "\n",
    "    df_res = pd.DataFrame(all_rows)\n",
    "    if not df_res.empty:\n",
    "        df_res = df_res.set_index(['Dataset', 'Optimizer', 'Activation'])\n",
    "        # Define column order for display\n",
    "        cols_order = ['Best MAE', 'Avg Accuracy', 'Std Accuracy', 'Avg Mae', 'Std Mae', \n",
    "                     'Avg Loss', 'Std Loss', 'Avg Time', 'Std Time', 'Avg Params']\n",
    "        # Only keep columns that exist in the dataframe\n",
    "        cols_order = [col for col in cols_order if col in df_res.columns]\n",
    "        df_res = df_res.reindex(columns=cols_order).sort_index()\n",
    "    \n",
    "    print(\"\\n--- Aggregated Benchmark Results ---\")\n",
    "    if best_mae:\n",
    "        print(\"\\nBest MAE for Regression Tasks:\")\n",
    "        for ds, info in best_mae.items():\n",
    "            print(f\"{ds}: {info['optimizer']} with {info['activation']} (MAE: {info['value']:.4f})\")\n",
    "    \n",
    "    pd.options.display.float_format = '{:.4f}'.format\n",
    "    print(df_res.dropna(axis=1, how='all') if not df_res.empty else \"No results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc0b21b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T08:01:32.829560Z",
     "iopub.status.busy": "2025-05-08T08:01:32.829367Z",
     "iopub.status.idle": "2025-05-08T12:58:27.860322Z",
     "shell.execute_reply": "2025-05-08T12:58:27.859481Z"
    },
    "papermill": {
     "duration": 17815.034538,
     "end_time": "2025-05-08T12:58:27.861554",
     "exception": false,
     "start_time": "2025-05-08T08:01:32.827016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective BATCH_SIZE: 16, EPOCHS: 50, NUM_RUNS: 1\n",
      "\n",
      "--- Starting Run 1/1 ---\n",
      "\n",
      "Dataset: UTKFaceAge (Task: regression)\n",
      "Parsing UTKFace metadata from: /kaggle/input/utkface-new/UTKFace\n",
      "Parsed 23708 image paths. Skipped 0 files.\n",
      "UTKFace splits: Train: 16595, Validation: 3556, Test: 3557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746691294.579221      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Optimizer: AdamW (Run 1)\n",
      "    Activation: OptimA\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746691361.118301      59 service.cc:148] XLA service 0x7dd8b0003010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1746691361.119097      59 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1746691365.949780      59 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   1/1038\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25:05:33\u001b[0m 87s/step - loss: 1433.4185 - mae: 30.7487"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746691386.404711      59 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 135ms/step - loss: 663.1389 - mae: 19.1174 - val_loss: 83.4814 - val_mae: 6.7262 - learning_rate: 1.0000e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m   1/1038\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:31\u001b[0m 146ms/step - loss: 113.6577 - mae: 8.8349"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_mae` which is not available. Available metrics are: loss,mae\n",
      "  current = self.get_monitor_value(logs)\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py:96: UserWarning: Learning rate reduction is conditioned on metric `val_mae` which is not available. Available metrics are: loss,mae,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 98.5598 - mae: 7.4664 - val_loss: 72.9784 - val_mae: 6.1999 - learning_rate: 1.0000e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 74.5841 - mae: 6.4369 - val_loss: 70.9566 - val_mae: 6.0678 - learning_rate: 1.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 56.2359 - mae: 5.6665 - val_loss: 69.2320 - val_mae: 5.9744 - learning_rate: 1.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 109ms/step - loss: 45.6441 - mae: 5.0944 - val_loss: 67.4712 - val_mae: 5.8963 - learning_rate: 1.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 36.6679 - mae: 4.5851 - val_loss: 66.1123 - val_mae: 5.8383 - learning_rate: 1.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 30.9583 - mae: 4.2299 - val_loss: 64.3153 - val_mae: 5.7133 - learning_rate: 1.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 109ms/step - loss: 25.4025 - mae: 3.8000 - val_loss: 64.8974 - val_mae: 5.7713 - learning_rate: 1.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 22.0447 - mae: 3.5328 - val_loss: 64.8485 - val_mae: 5.7120 - learning_rate: 1.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 109ms/step - loss: 20.8240 - mae: 3.3889 - val_loss: 62.1876 - val_mae: 5.6465 - learning_rate: 1.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 18.4302 - mae: 3.1904 - val_loss: 62.1341 - val_mae: 5.6473 - learning_rate: 1.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 16.4968 - mae: 3.0417 - val_loss: 61.0408 - val_mae: 5.5770 - learning_rate: 1.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 16.3075 - mae: 3.0142 - val_loss: 62.1326 - val_mae: 5.6728 - learning_rate: 1.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 15.5957 - mae: 2.9350 - val_loss: 61.1540 - val_mae: 5.6359 - learning_rate: 1.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 14.4122 - mae: 2.8201 - val_loss: 61.4953 - val_mae: 5.5368 - learning_rate: 1.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 13.7918 - mae: 2.7713 - val_loss: 59.3622 - val_mae: 5.4709 - learning_rate: 1.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 108ms/step - loss: 12.9067 - mae: 2.6587 - val_loss: 60.2940 - val_mae: 5.4563 - learning_rate: 1.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 13.1163 - mae: 2.6752 - val_loss: 60.7318 - val_mae: 5.6354 - learning_rate: 1.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 108ms/step - loss: 12.2119 - mae: 2.6035 - val_loss: 58.2515 - val_mae: 5.4274 - learning_rate: 1.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 11.6646 - mae: 2.5384 - val_loss: 58.9612 - val_mae: 5.4111 - learning_rate: 1.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 11.5093 - mae: 2.5084 - val_loss: 58.2624 - val_mae: 5.4648 - learning_rate: 1.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 11.2626 - mae: 2.4627 - val_loss: 58.0163 - val_mae: 5.4709 - learning_rate: 1.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 11.2557 - mae: 2.4668 - val_loss: 58.9728 - val_mae: 5.4124 - learning_rate: 1.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 11.2156 - mae: 2.4799 - val_loss: 57.6576 - val_mae: 5.3693 - learning_rate: 1.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 108ms/step - loss: 11.0294 - mae: 2.4498 - val_loss: 58.5337 - val_mae: 5.3902 - learning_rate: 1.0000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Evaluating model...\n",
      "      Loss: 60.2574, Mae: 5.4887, Time: 2943.32s, Params: 24112518\n",
      "    Activation: OptimALinear\n",
      "Epoch 1/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 138ms/step - loss: 620.1153 - mae: 18.2667 - val_loss: 96.2119 - val_mae: 7.1713 - learning_rate: 1.0000e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 110ms/step - loss: 104.9634 - mae: 7.7568 - val_loss: 80.6569 - val_mae: 6.5082 - learning_rate: 1.0000e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 76.7185 - mae: 6.6166 - val_loss: 80.8217 - val_mae: 6.5979 - learning_rate: 1.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 111ms/step - loss: 60.4411 - mae: 5.9068 - val_loss: 76.0239 - val_mae: 6.2599 - learning_rate: 1.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 113ms/step - loss: 50.1647 - mae: 5.4017 - val_loss: 71.2377 - val_mae: 6.1474 - learning_rate: 1.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 41.5237 - mae: 4.8698 - val_loss: 69.8247 - val_mae: 6.0230 - learning_rate: 1.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 36.4809 - mae: 4.5480 - val_loss: 69.2424 - val_mae: 5.9680 - learning_rate: 1.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 32.7162 - mae: 4.3175 - val_loss: 70.0273 - val_mae: 6.0381 - learning_rate: 1.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 27.9168 - mae: 3.9691 - val_loss: 70.0866 - val_mae: 6.1530 - learning_rate: 1.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 28.2146 - mae: 3.9397 - val_loss: 66.6435 - val_mae: 5.9014 - learning_rate: 1.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 23.4954 - mae: 3.6104 - val_loss: 68.5122 - val_mae: 6.1350 - learning_rate: 1.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 21.1485 - mae: 3.4179 - val_loss: 68.4427 - val_mae: 5.8887 - learning_rate: 1.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 110ms/step - loss: 21.1001 - mae: 3.4129 - val_loss: 69.0970 - val_mae: 6.1258 - learning_rate: 1.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 20.3941 - mae: 3.3206 - val_loss: 64.4941 - val_mae: 5.8055 - learning_rate: 1.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 18.5753 - mae: 3.2045 - val_loss: 63.8775 - val_mae: 5.7412 - learning_rate: 1.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 17.9284 - mae: 3.1079 - val_loss: 66.5002 - val_mae: 5.9182 - learning_rate: 1.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 17.4014 - mae: 3.0911 - val_loss: 64.7602 - val_mae: 5.8517 - learning_rate: 1.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 16.1874 - mae: 2.9828 - val_loss: 64.0981 - val_mae: 5.7461 - learning_rate: 1.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 108ms/step - loss: 15.0018 - mae: 2.8469 - val_loss: 62.6883 - val_mae: 5.7082 - learning_rate: 1.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 15.8299 - mae: 2.9332 - val_loss: 62.4936 - val_mae: 5.6766 - learning_rate: 1.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 14.4840 - mae: 2.7638 - val_loss: 63.4695 - val_mae: 5.7542 - learning_rate: 1.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 14.5627 - mae: 2.7717 - val_loss: 62.3024 - val_mae: 5.6881 - learning_rate: 1.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 13.7632 - mae: 2.7202 - val_loss: 62.8480 - val_mae: 5.7838 - learning_rate: 1.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 13.5705 - mae: 2.6809 - val_loss: 61.6519 - val_mae: 5.6679 - learning_rate: 1.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 13.3702 - mae: 2.6676 - val_loss: 62.6256 - val_mae: 5.6940 - learning_rate: 1.0000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Evaluating model...\n",
      "      Loss: 63.1869, Mae: 5.7595, Time: 2959.84s, Params: 24112518\n",
      "    Activation: ReLU\n",
      "Epoch 1/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 136ms/step - loss: 722.2872 - mae: 20.2314 - val_loss: 90.1433 - val_mae: 7.0564 - learning_rate: 1.0000e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 104.9250 - mae: 7.7174 - val_loss: 78.0691 - val_mae: 6.4549 - learning_rate: 1.0000e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 80.0087 - mae: 6.7832 - val_loss: 74.0817 - val_mae: 6.2506 - learning_rate: 1.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 111ms/step - loss: 61.5273 - mae: 5.9797 - val_loss: 72.2881 - val_mae: 6.1224 - learning_rate: 1.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 53.2779 - mae: 5.5234 - val_loss: 68.9693 - val_mae: 5.9939 - learning_rate: 1.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 109ms/step - loss: 42.3884 - mae: 4.9857 - val_loss: 67.1268 - val_mae: 5.8921 - learning_rate: 1.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 37.9849 - mae: 4.6637 - val_loss: 71.2005 - val_mae: 6.1756 - learning_rate: 1.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 32.5509 - mae: 4.3039 - val_loss: 71.1402 - val_mae: 6.0390 - learning_rate: 1.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 28.5201 - mae: 4.0058 - val_loss: 67.1629 - val_mae: 5.9595 - learning_rate: 1.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 27.9043 - mae: 3.9487 - val_loss: 65.6522 - val_mae: 5.8183 - learning_rate: 1.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 25.2684 - mae: 3.7642 - val_loss: 69.0816 - val_mae: 6.0643 - learning_rate: 1.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 23.3175 - mae: 3.5949 - val_loss: 63.9507 - val_mae: 5.7684 - learning_rate: 1.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 111ms/step - loss: 21.4875 - mae: 3.4679 - val_loss: 66.5741 - val_mae: 5.7729 - learning_rate: 1.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 111ms/step - loss: 20.4840 - mae: 3.3682 - val_loss: 63.8522 - val_mae: 5.8045 - learning_rate: 1.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 111ms/step - loss: 19.1306 - mae: 3.2789 - val_loss: 61.5225 - val_mae: 5.5942 - learning_rate: 1.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 111ms/step - loss: 18.2958 - mae: 3.1862 - val_loss: 61.6180 - val_mae: 5.6003 - learning_rate: 1.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 111ms/step - loss: 18.1277 - mae: 3.1638 - val_loss: 61.1205 - val_mae: 5.5975 - learning_rate: 1.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 17.5498 - mae: 3.1090 - val_loss: 59.8374 - val_mae: 5.5448 - learning_rate: 1.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 17.1152 - mae: 3.0516 - val_loss: 60.8856 - val_mae: 5.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 110ms/step - loss: 16.8401 - mae: 3.0193 - val_loss: 60.7769 - val_mae: 5.6084 - learning_rate: 1.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 109ms/step - loss: 16.0058 - mae: 2.9658 - val_loss: 60.2988 - val_mae: 5.5602 - learning_rate: 1.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 110ms/step - loss: 15.3633 - mae: 2.8918 - val_loss: 59.1616 - val_mae: 5.4593 - learning_rate: 1.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 15.3623 - mae: 2.8989 - val_loss: 59.9586 - val_mae: 5.5096 - learning_rate: 1.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 110ms/step - loss: 15.4023 - mae: 2.9081 - val_loss: 60.0219 - val_mae: 5.5188 - learning_rate: 1.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 14.6322 - mae: 2.8228 - val_loss: 60.2372 - val_mae: 5.5097 - learning_rate: 1.0000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Evaluating model...\n",
      "      Loss: 61.6305, Mae: 5.5988, Time: 2962.76s, Params: 24112513\n",
      "    Activation: ELU\n",
      "Epoch 1/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 139ms/step - loss: 618.8483 - mae: 18.2114 - val_loss: 84.1491 - val_mae: 6.7985 - learning_rate: 1.0000e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 96.1002 - mae: 7.3937 - val_loss: 77.3699 - val_mae: 6.3899 - learning_rate: 1.0000e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 73.2679 - mae: 6.4876 - val_loss: 73.5327 - val_mae: 6.2260 - learning_rate: 1.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 57.1586 - mae: 5.7328 - val_loss: 73.0510 - val_mae: 6.2265 - learning_rate: 1.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 110ms/step - loss: 43.8488 - mae: 5.0670 - val_loss: 77.8531 - val_mae: 6.5057 - learning_rate: 1.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 37.7836 - mae: 4.6533 - val_loss: 69.1855 - val_mae: 5.9829 - learning_rate: 1.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 109ms/step - loss: 32.1255 - mae: 4.3088 - val_loss: 68.4276 - val_mae: 5.9314 - learning_rate: 1.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 27.2576 - mae: 3.9400 - val_loss: 67.4972 - val_mae: 5.9401 - learning_rate: 1.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 25.7544 - mae: 3.8396 - val_loss: 67.7186 - val_mae: 5.9807 - learning_rate: 1.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 22.5982 - mae: 3.5837 - val_loss: 66.5625 - val_mae: 5.7683 - learning_rate: 1.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 109ms/step - loss: 21.1262 - mae: 3.4538 - val_loss: 65.9275 - val_mae: 5.9004 - learning_rate: 1.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 20.1693 - mae: 3.3338 - val_loss: 63.2729 - val_mae: 5.7335 - learning_rate: 1.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 111ms/step - loss: 18.3443 - mae: 3.2136 - val_loss: 62.6772 - val_mae: 5.7075 - learning_rate: 1.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 17.1261 - mae: 3.0767 - val_loss: 66.4717 - val_mae: 5.9786 - learning_rate: 1.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 16.7764 - mae: 3.0635 - val_loss: 63.6762 - val_mae: 5.7909 - learning_rate: 1.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 16.5023 - mae: 3.0362 - val_loss: 63.6553 - val_mae: 5.8074 - learning_rate: 1.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 14.1905 - mae: 2.7902 - val_loss: 60.8591 - val_mae: 5.5373 - learning_rate: 1.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 14.6950 - mae: 2.8498 - val_loss: 61.5766 - val_mae: 5.6439 - learning_rate: 1.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 14.2134 - mae: 2.7874 - val_loss: 60.9607 - val_mae: 5.5204 - learning_rate: 1.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 13.5103 - mae: 2.7119 - val_loss: 61.1665 - val_mae: 5.6305 - learning_rate: 1.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 109ms/step - loss: 12.5788 - mae: 2.6011 - val_loss: 59.8349 - val_mae: 5.5437 - learning_rate: 1.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 111ms/step - loss: 12.2587 - mae: 2.5802 - val_loss: 60.5434 - val_mae: 5.5439 - learning_rate: 1.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 12.0298 - mae: 2.5679 - val_loss: 59.4762 - val_mae: 5.4789 - learning_rate: 1.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 11.8362 - mae: 2.5507 - val_loss: 59.6844 - val_mae: 5.5138 - learning_rate: 1.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 108ms/step - loss: 11.8382 - mae: 2.5317 - val_loss: 60.5902 - val_mae: 5.6031 - learning_rate: 1.0000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Evaluating model...\n",
      "      Loss: 60.8572, Mae: 5.5788, Time: 2984.61s, Params: 24112513\n",
      "    Activation: Swish\n",
      "Epoch 1/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 135ms/step - loss: 690.9882 - mae: 19.6529 - val_loss: 84.3450 - val_mae: 6.8325 - learning_rate: 1.0000e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 104.1813 - mae: 7.6459 - val_loss: 78.2783 - val_mae: 6.4533 - learning_rate: 1.0000e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 75.4541 - mae: 6.6098 - val_loss: 76.3303 - val_mae: 6.3133 - learning_rate: 1.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 62.7253 - mae: 6.0514 - val_loss: 73.8031 - val_mae: 6.1461 - learning_rate: 1.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 109ms/step - loss: 52.5042 - mae: 5.5289 - val_loss: 71.4312 - val_mae: 6.1276 - learning_rate: 1.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 111ms/step - loss: 42.4658 - mae: 4.9960 - val_loss: 68.2944 - val_mae: 5.9702 - learning_rate: 1.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 35.8672 - mae: 4.5366 - val_loss: 69.5133 - val_mae: 6.0310 - learning_rate: 1.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 31.5509 - mae: 4.2485 - val_loss: 68.7084 - val_mae: 6.0138 - learning_rate: 1.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 29.3624 - mae: 4.0419 - val_loss: 64.7063 - val_mae: 5.7591 - learning_rate: 1.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 27.1644 - mae: 3.9455 - val_loss: 66.6815 - val_mae: 5.9401 - learning_rate: 1.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 24.4127 - mae: 3.6533 - val_loss: 65.2734 - val_mae: 5.8102 - learning_rate: 1.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 22.1980 - mae: 3.5340 - val_loss: 65.6797 - val_mae: 5.7627 - learning_rate: 1.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 110ms/step - loss: 21.7075 - mae: 3.4521 - val_loss: 62.7354 - val_mae: 5.7064 - learning_rate: 1.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 19.4463 - mae: 3.2887 - val_loss: 66.4262 - val_mae: 5.7723 - learning_rate: 1.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 18.2540 - mae: 3.1928 - val_loss: 62.3616 - val_mae: 5.6320 - learning_rate: 1.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 17.4702 - mae: 3.0807 - val_loss: 62.4735 - val_mae: 5.7087 - learning_rate: 1.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 110ms/step - loss: 17.1409 - mae: 3.0576 - val_loss: 61.8481 - val_mae: 5.6085 - learning_rate: 1.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 16.9722 - mae: 3.0425 - val_loss: 60.7385 - val_mae: 5.5565 - learning_rate: 1.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 15.6301 - mae: 2.9177 - val_loss: 61.4477 - val_mae: 5.6213 - learning_rate: 1.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 15.6986 - mae: 2.9041 - val_loss: 60.6546 - val_mae: 5.5902 - learning_rate: 1.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 15.9590 - mae: 2.9512 - val_loss: 60.8920 - val_mae: 5.5586 - learning_rate: 1.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 110ms/step - loss: 15.6114 - mae: 2.9095 - val_loss: 61.9020 - val_mae: 5.6910 - learning_rate: 1.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 14.8017 - mae: 2.8188\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 2.9999999242136253e-06.\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 14.8011 - mae: 2.8187 - val_loss: 62.1794 - val_mae: 5.7228 - learning_rate: 1.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 3.0000e-06\n",
      "Epoch 47/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 109ms/step - loss: 13.8623 - mae: 2.7323 - val_loss: 59.5015 - val_mae: 5.5284 - learning_rate: 3.0000e-06\n",
      "Epoch 48/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 3.0000e-06\n",
      "Epoch 49/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 13.3278 - mae: 2.6932 - val_loss: 58.9636 - val_mae: 5.4653 - learning_rate: 3.0000e-06\n",
      "Epoch 50/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 3.0000e-06\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Evaluating model...\n",
      "      Loss: 61.0905, Mae: 5.6110, Time: 2976.98s, Params: 24112513\n",
      "    Activation: GeLU\n",
      "Epoch 1/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 133ms/step - loss: 724.7604 - mae: 20.1408 - val_loss: 85.8607 - val_mae: 6.8789 - learning_rate: 1.0000e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 104.2038 - mae: 7.7041 - val_loss: 80.9464 - val_mae: 6.5775 - learning_rate: 1.0000e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 80.1671 - mae: 6.7991 - val_loss: 74.0802 - val_mae: 6.1990 - learning_rate: 1.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 60.0743 - mae: 5.9062 - val_loss: 69.9797 - val_mae: 6.0543 - learning_rate: 1.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 52.9737 - mae: 5.5099 - val_loss: 73.7078 - val_mae: 6.2643 - learning_rate: 1.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 46.1819 - mae: 5.1887 - val_loss: 69.2063 - val_mae: 6.0161 - learning_rate: 1.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 37.8146 - mae: 4.6491 - val_loss: 65.8400 - val_mae: 5.8541 - learning_rate: 1.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 32.6145 - mae: 4.3055 - val_loss: 66.2570 - val_mae: 5.8564 - learning_rate: 1.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 28.2315 - mae: 4.0289 - val_loss: 65.1699 - val_mae: 5.8076 - learning_rate: 1.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 26.7987 - mae: 3.8550 - val_loss: 69.0849 - val_mae: 6.0899 - learning_rate: 1.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 23.9857 - mae: 3.6596 - val_loss: 63.1895 - val_mae: 5.6899 - learning_rate: 1.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 22.9614 - mae: 3.5796 - val_loss: 62.3505 - val_mae: 5.6673 - learning_rate: 1.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 20.6031 - mae: 3.3945 - val_loss: 62.6297 - val_mae: 5.6963 - learning_rate: 1.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 20.8398 - mae: 3.3822 - val_loss: 61.2804 - val_mae: 5.6306 - learning_rate: 1.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 19.7508 - mae: 3.2795 - val_loss: 61.4179 - val_mae: 5.6162 - learning_rate: 1.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 17.7940 - mae: 3.1290 - val_loss: 61.9989 - val_mae: 5.7246 - learning_rate: 1.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 17.4460 - mae: 3.1027 - val_loss: 60.7885 - val_mae: 5.5419 - learning_rate: 1.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 16.6107 - mae: 3.0311 - val_loss: 60.2899 - val_mae: 5.5914 - learning_rate: 1.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 16.5769 - mae: 3.0081 - val_loss: 59.8279 - val_mae: 5.5557 - learning_rate: 1.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 16.9380 - mae: 3.0223 - val_loss: 59.4936 - val_mae: 5.5146 - learning_rate: 1.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 15.5588 - mae: 2.9018 - val_loss: 58.7042 - val_mae: 5.4811 - learning_rate: 1.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 15.3501 - mae: 2.8581 - val_loss: 59.2279 - val_mae: 5.5072 - learning_rate: 1.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 107ms/step - loss: 15.8925 - mae: 2.9070 - val_loss: 59.7947 - val_mae: 5.6015 - learning_rate: 1.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 108ms/step - loss: 15.3274 - mae: 2.8631 - val_loss: 58.3711 - val_mae: 5.4615 - learning_rate: 1.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 109ms/step - loss: 13.7718 - mae: 2.7582 - val_loss: 60.3240 - val_mae: 5.6712 - learning_rate: 1.0000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m1038/1038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34us/step - loss: 0.0000e+00 - mae: 0.0000e+00 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Evaluating model...\n",
      "      Loss: 61.4983, Mae: 5.6555, Time: 2896.00s, Params: 24112513\n",
      "\n",
      "--- Aggregated Benchmark Results ---\n",
      "\n",
      "Best MAE for Regression Tasks:\n",
      "UTKFaceAge: AdamW with OptimA (MAE: 5.4887)\n",
      "                                  Best MAE  Avg Mae  Std Mae  Avg Loss  \\\n",
      "Dataset    Optimizer Activation                                          \n",
      "UTKFaceAge AdamW     ELU                     5.5788   0.0000   60.8572   \n",
      "                     GeLU                    5.6555   0.0000   61.4983   \n",
      "                     OptimA              ★   5.4887   0.0000   60.2574   \n",
      "                     OptimALinear            5.7595   0.0000   63.1869   \n",
      "                     ReLU                    5.5988   0.0000   61.6305   \n",
      "                     Swish                   5.6110   0.0000   61.0905   \n",
      "\n",
      "                                   Std Loss  Avg Time  Std Time  Avg Params  \n",
      "Dataset    Optimizer Activation                                              \n",
      "UTKFaceAge AdamW     ELU             0.0000 2984.6072    0.0000    24112513  \n",
      "                     GeLU            0.0000 2895.9952    0.0000    24112513  \n",
      "                     OptimA          0.0000 2943.3229    0.0000    24112518  \n",
      "                     OptimALinear    0.0000 2959.8438    0.0000    24112518  \n",
      "                     ReLU            0.0000 2962.7553    0.0000    24112513  \n",
      "                     Swish           0.0000 2976.9817    0.0000    24112513  \n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define datasets to benchmark\n",
    "    datasets_run_cfg = {\n",
    "        # Example: Uncomment to run MNIST classification\n",
    "        # \"MNIST\": {\"load_func\": mnist.load_data, \"type\": \"image\", \"task\": \"classification\", \"model_type\": \"cnn\"},\n",
    "        # Example: Uncomment to run CIFAR-10 classification\n",
    "        # \"CIFAR-10\": {\"load_func\": cifar10.load_data, \"type\": \"image\", \"task\": \"classification\", \"model_type\": \"cnn\"},\n",
    "        # Example: Uncomment to run Rotated MNIST regression\n",
    "        # \"RotatedMNIST\": {\"load_func\": load_and_prepare_rotated_mnist, \"type\": \"image\", \"task\": \"regression\", \"model_type\": \"cnn\"},\n",
    "    }\n",
    "    \n",
    "    # Add UTKFaceAge if directory exists and is not empty\n",
    "    if os.path.isdir(UTKFACE_DIR) and os.listdir(UTKFACE_DIR):\n",
    "         datasets_run_cfg[\"UTKFaceAge\"] = {\n",
    "             \"load_func\": parse_utkface_metadata, # Function to parse paths/labels\n",
    "             \"type\": \"image\", \"task\": \"regression\", \"model_type\": \"resnet50\"\n",
    "         }\n",
    "    else:\n",
    "        print(f\"WARNING: UTKFace directory '{UTKFACE_DIR}' not found or is empty. Skipping UTKFaceAge dataset.\")\n",
    "    \n",
    "    if not datasets_run_cfg: print(\"No datasets configured to run. Exiting.\"); exit()\n",
    "\n",
    "    # Define activation functions to compare\n",
    "    activations_run_cfg = {'OptimA': OptimA, 'OptimALinear': OptimALinear, \n",
    "                           'ReLU': 'relu', 'ELU': 'elu', 'Swish': 'swish', 'GeLU': 'gelu'}\n",
    "    # Define optimizers to compare (using lambdas for fresh instances)\n",
    "    optimizers_run_cfg = {\n",
    "                          'AdamW': lambda: AdamW(learning_rate=LEARNING_RATE, beta_1=0.95, beta_2=0.999, amsgrad=True)\n",
    "                         }\n",
    "\n",
    "    print(f\"Effective BATCH_SIZE: {BATCH_SIZE}, EPOCHS: {EPOCHS}, NUM_RUNS: {NUM_RUNS}\")\n",
    "\n",
    "    # Run the experiments\n",
    "    final_data, all_hists = run_experiment(\n",
    "        datasets_run_cfg, activations_run_cfg, optimizers_run_cfg,\n",
    "        num_runs=NUM_RUNS, epochs_local=EPOCHS, batch_size_local=BATCH_SIZE, patience_local=PATIENCE\n",
    "    )\n",
    "    # Display aggregated results and plots\n",
    "    aggregate_and_display_results(final_data, datasets_run_cfg, all_hists, NUM_RUNS)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 44109,
     "isSourceIdPinned": true,
     "sourceId": 78156,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17843.517663,
   "end_time": "2025-05-08T12:58:38.333893",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-08T08:01:14.816230",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
